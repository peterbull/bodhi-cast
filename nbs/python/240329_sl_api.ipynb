{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LOCAL_AIRFLOW_PG_URI' from 'utils.utils' (/home/peter-legion-wsl2/peter-projects/bodhi-cast/nbs/python/utils/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_engine, select, text, and_\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sessionmaker, declarative_base\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOCAL_AIRFLOW_PG_URI, LOCAL_PG_URI\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LOCAL_AIRFLOW_PG_URI' from 'utils.utils' (/home/peter-legion-wsl2/peter-projects/bodhi-cast/nbs/python/utils/utils.py)"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import List, Dict, Optional\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import aiohttp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pendulum\n",
    "import logging\n",
    "import requests\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "from sqlalchemy import create_engine, select, text, and_\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from utils.utils import LOCAL_AIRFLOW_PG_URI, LOCAL_PG_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(LOCAL_AIRFLOW_PG_URI)\n",
    "SessionLocal = sessionmaker(bind=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "See `240220_sl_surf_spots.ipynb` for spot getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://services.surfline.com/taxonomy?type=taxonomy&id=58f7ed51dadb30820bb3879c&maxDepth=0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You will not get Surfline forecast data without a valid Surfline premium login. Add your credentials to `.env.development`:\n",
    "  ```\n",
    "  SURFLINE_EMAIL=xxx\n",
    "  SURFLINE_PASSWORD=yyy\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Requests\n",
    "\n",
    "`https://services.surfline.com/kbyg/spots/forecasts/{type}?{params}`\n",
    "\n",
    "\n",
    "Type|Data\n",
    "----|----\n",
    "rating|array of human-readable and numeric (0-6) ratings\n",
    "wave|array of min/max sizes & optimal scores\n",
    "wind|array of wind directions/speeds & optimal scores\n",
    "tides|array of types & heights\n",
    "weather|array of sunrise/set times, array of temperatures/weather conditions\n",
    "\n",
    "Param|Values|Effect\n",
    "-----|------|------\n",
    "spotId|string|Surfline spot id that you want data for. A typical Surfline URL is `https://www.surfline.com/surf-report/venice-breakwater/590927576a2e4300134fbed8` where `590927576a2e4300134fbed8` is the `spotId`\n",
    "days|integer|Number of forecast days to get (Max 6 w/o access token, Max 17 w/ premium token)\n",
    "intervalHours|integer|Minimum of 1 (hour)\n",
    "maxHeights|boolean|`true` seems to remove min & optimal values from the wave data output\n",
    "sds|boolean|If true, use the new LOTUS forecast engine\n",
    "accesstoken|string|Auth token to get premium data access (optional)\n",
    "\n",
    "Anywhere there is an `optimalScore` the value can be interpreted as follows:\n",
    "\n",
    "Value|Meaning\n",
    "-----|-------\n",
    "0|Suboptimal\n",
    "1|Good\n",
    "2|Optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = [\"rating\", \"wave\", \"wind\", \"tides\", \"weather\"]\n",
    "params = [\"spotId\", \"days\", \"intervalHours\", \"maxHeights\", \"sds\", \"accesstoken\"]\n",
    "base = \"https://services.surfline.com/kbyg/spots/forecasts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path('./data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datapath/'spot_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the spot `id` for 1st Street Jetty in Va Beach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_id = df[df['names'].str.contains('1st Street Jetty', case=False, na=False)]['ids'].values[0]\n",
    "jetty_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_params = {params[0]: jetty_id}\n",
    "ex_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surfline seems to change their spot IDs periodically. Check a spot on the website and pass the objectId from the url as a param to debug if this is the case. If they've changed you'll need to run the notebook `240220_sl_surf_spots.ipynb` as mentioned above to refresh the spots dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_params = {params[0]: \"584204214e65fad6a7709ce7\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(f\"{base}/{types[0]}\", params=ex_params)\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_json = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_day_json = res.json()\n",
    "if 'data' in four_day_json and 'rating' in four_day_json['data']:\n",
    "    four_day_json['data']['rating'] = four_day_json['data']['rating'][:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cull_extra_days(full_json):\n",
    "    if 'data' in full_json and 'rating' in full_json['data']:\n",
    "        full_json['data']['rating'] = full_json['data']['rating'][:24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop extra days of forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cull_extra_days(four_day_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(four_day_json['data']['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a unix timestamp -> utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum.from_timestamp(rating_json['data']['rating'][0]['timestamp'], 'UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum.from_timestamp(rating_json['data']['rating'][int(72 / 3)-1]['timestamp'], 'UTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `utcOffset` field seems to be aware that I'm working in EST currently. Either that or it's the time coding for the spot itself.\n",
    "\n",
    "Let's check a west coast spot to confirm how this is handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_jolla_id = df[df['names'].str.contains(\"La Jolla\", case=False, na=False)]['ids'].values[0]\n",
    "la_jolla_dict = {params[0]: la_jolla_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_jolla_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum.now(\"utc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {\"spot_id\": \"test\", \"spot_name\": \"test_2\", \"date\": pendulum.now(\"utc\"), \"forecast\": four_day_json}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SlApiEndpoints(Enum):\n",
    "    RATING = 'rating'\n",
    "    WAVE = 'wave'\n",
    "    WIND = 'wind'\n",
    "    TIDES = 'tides'\n",
    "    WEATHER = 'weather'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SlApiParams(Enum):\n",
    "    SPOT_ID = 'spotId'\n",
    "    DAYS = 'days'\n",
    "    INTERVAL_HOURS = 'intervalHours'\n",
    "    MAX_HEIGHTS = 'maxHeights'\n",
    "    SDS = 'sds'\n",
    "    ACCESSTOKEN = 'accesstoken'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_from_sl_api(endpoint: SlApiEndpoints, param_type: SlApiParams, param: str):\n",
    "    base_url = \"https://services.surfline.com/kbyg/spots/forecasts\"\n",
    "    res = requests.get(f\"{base_url}/{endpoint}\", params={param_type: param})\n",
    "    data = res.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = fetch_from_sl_api(SlApiEndpoints.RATING.value, SlApiParams.SPOT_ID.value, param=jetty_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_ratings = []\n",
    "for spot_id, spot_name in df[['ids', 'names']][:3].values:\n",
    "    res = requests.get(f\"{base}/rating\", params={'spotId': spot_id})\n",
    "    data = res.json()\n",
    "    cull_extra_days(data)\n",
    "    current_date = pendulum.now(\"utc\")\n",
    "    utc_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "    data['spot_id'] = spot_id\n",
    "    data['spot_name'] = spot_name\n",
    "    data['utc_fetch_date'] = utc_date\n",
    "    spot_ratings.append(data)\n",
    "    # time.sleep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum.from_timestamp(rating_json['data']['rating'][0]['timestamp'], 'UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.json_normalize(spot_ratings, record_path=['data', 'rating'], meta=['spot_id', 'spot_name', 'utc_fetch_date'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df['timestamp'] = ratings_df['timestamp'].apply(lambda x: pendulum.from_timestamp(x).to_datetime_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so it looks like each spot's forecast starts at 12am *local time*, with the timestamp for that time in unix. To figure out the flat `UTC` time for each spot you can just apply the `utcOffset` that is included in response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df['timestamp'] = pd.to_datetime(ratings_df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df['timestamp_utc'] = ratings_df.apply(lambda row: row['timestamp'] + pd.Timedelta(hours=row['utcOffset']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SessionLocal() as db:\n",
    "    matching_spots = []\n",
    "    for spot in df['names']:\n",
    "        stmt = text(\"\"\"select * from spots where spot_name like :spot\"\"\")\n",
    "        result = db.execute(stmt, {\"spot\": spot}).fetchall()\n",
    "        if len(result) > 0:\n",
    "            matching_spots.append(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matching_spots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_waves = fetch_from_sl_api(SlApiEndpoints.WAVE.value, SlApiParams.SPOT_ID.value, jetty_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_waves['associated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_waves['associated']['spotId'] = jetty_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_meta_df = pd.json_normalize(jetty_waves)\n",
    "jetty_meta_df.drop(['permissions.violations', 'permissions.data', 'data.wave'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_waves['data']['spotId'] = jetty_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_wave_df = pd.json_normalize(\n",
    "    jetty_waves, record_path=[\"data\", \"wave\"], meta=[[\"data\", \"spotId\"]]\n",
    ")\n",
    "jetty_wave_df.drop(\"swells\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_wave_df.rename(columns={\"power\": \"wave_power\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_swell_df = pd.json_normalize(\n",
    "    jetty_waves,\n",
    "    record_path=['data', 'wave', 'swells'],\n",
    "    meta=[['data', 'wave', 'timestamp'], ['data', 'spotId']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_swell_df['swells_idx'] = jetty_swell_df.groupby('data.wave.timestamp').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_swell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_swell_df.rename({\"power\": 'swell_power'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_swell_df['data.wave.timestamp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_wave_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetty_swell_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_waves_df = pd.merge(\n",
    "    jetty_wave_df,\n",
    "    jetty_swell_df,\n",
    "    how=\"inner\",\n",
    "    left_on=[\"timestamp\", \"data.spotId\"],\n",
    "    right_on=[\"data.wave.timestamp\", 'data.spotId'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_waves_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_waves_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(jetty_meta_df, combined_waves_df, how='cross')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurflineSpots:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.state_ids = []\n",
    "        self.state_urls = []\n",
    "        self.state_data = []\n",
    "        self.county_data = []\n",
    "        self.region_data = [] \n",
    "        self.spot_ids = []\n",
    "        self.spot_names = []\n",
    "        self.spot_address = []\n",
    "        self.spot_lon = []\n",
    "        self.spot_lat = []\n",
    "        self.spot_urls = []\n",
    "\n",
    "    def _update_states(self):\n",
    "        response = requests.get(\n",
    "            \"https://services.surfline.com/taxonomy?type=taxonomy&id=58f7ed51dadb30820bb3879c&maxDepth=0\"\n",
    "        )\n",
    "        json_data = response.json()\n",
    "        json_contains = json_data[\"contains\"]\n",
    "        for x in json_contains:\n",
    "            self.states.append(x[\"name\"])\n",
    "            self.state_ids.append(x[\"_id\"])\n",
    "        \n",
    "        for state_id in self.state_ids:\n",
    "            self.state_urls.append(\"https://services.surfline.com/taxonomy?type=taxonomy&id=\" + state_id + \"&maxDepth=0\")\n",
    "\n",
    "\n",
    "    async def fetch_url(self, url, session):\n",
    "        async with session.get(url) as response:\n",
    "            return await response.json()\n",
    "\n",
    "\n",
    "    async def fetch_all_urls(self, target):\n",
    "        data = []\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = []\n",
    "            for url in self.state_urls:\n",
    "                tasks.append(self.fetch_url(url, session))\n",
    "            data = await asyncio.gather(*tasks)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def update_data(self, data_target: List[str], attr_target):\n",
    "        data = asyncio.run(self.fetch_all_urls(data_target))\n",
    "        setattr(self, attr_target, data)\n",
    "    \n",
    "\n",
    "    def process_spots(self):\n",
    "        if len(self.states) == 0:\n",
    "            self._update_states() \n",
    "        \n",
    "        self.update_data(self.state_urls, \"state_data\")\n",
    "        logging.info(\"spots\")\n",
    "\n",
    "\n",
    "\n",
    "        county_ids = []    \n",
    "        for state in self.state_data:\n",
    "            state_contains = state['contains']\n",
    "            for y in state_contains:\n",
    "                county_ids.append(y['_id'])\n",
    "        \n",
    "        county_urls = []\n",
    "        for county_id in county_ids:\n",
    "            county_urls.append(\"https://services.surfline.com/taxonomy?type=taxonomy&id=\" + county_id + \"&maxDepth=0\")\n",
    "\n",
    "        self.update_data(county_urls, \"county_data\")\n",
    "\n",
    "        region_ids = []\n",
    "        region_names = []\n",
    "        for county in self.county_data:\n",
    "            county_contains = county['contains']\n",
    "            for z in county_contains:\n",
    "                region_ids.append(z['_id'])\n",
    "                region_names.append(z['name'])\n",
    "\n",
    "        region_urls = []\n",
    "        for region_id in region_ids:\n",
    "            region_urls.append(\"https://services.surfline.com/taxonomy?type=taxonomy&id=\" + region_id + \"&maxDepth=0\")\n",
    "\n",
    "        self.update_data(region_urls, \"region_data\")\n",
    "\n",
    "\n",
    "        for region in self.region_data:\n",
    "            region_contains = region['contains']\n",
    "            if len(region_contains) == 0:\n",
    "                self.spot_ids.append(region.get(\"spot\", \"\"))\n",
    "                self.spot_names.append(region.get(\"name\", \"\"))\n",
    "                self.spot_address.append(\"\")\n",
    "                region_associated = region['associated']\n",
    "                region_links = region_associated['links']\n",
    "                region_location = region['location']\n",
    "                region_coordinates = region_location['coordinates']\n",
    "                self.spot_lon.append(region_coordinates[0])\n",
    "                self.spot_lat.append(region_coordinates[1])\n",
    "                for i in region_links:\n",
    "                    if i['key'] == \"www\":\n",
    "                        self.spot_urls.append(i['href'])\n",
    "\n",
    "        df = pd.DataFrame({\"ids\": self.spot_ids, \"names\": self.spot_names, \"lon\": self.spot_lon, \"lat\": self.spot_lat, \"urls\": self.spot_urls})\n",
    "        return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots = SurflineSpots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots.process_spots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sl_spots():\n",
    "    response = requests.get(\n",
    "        \"https://services.surfline.com/taxonomy?type=taxonomy&id=58f7ed51dadb30820bb3879c&maxDepth=0\"\n",
    "    )\n",
    "    json_data = response.json()\n",
    "    json_contains = json_data[\"contains\"]\n",
    "    states = []\n",
    "    state_ids = []\n",
    "    state_urls = []\n",
    "    for x in json_contains:\n",
    "        states.append(x[\"name\"])\n",
    "        state_ids.append(x[\"_id\"])\n",
    "    \n",
    "\n",
    "    for state_id in state_ids:\n",
    "        state_urls.append(\"https://services.surfline.com/taxonomy?type=taxonomy&id=\" + state_id + \"&maxDepth=0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sl_wave_data(data: Dict) -> pd.DataFrame:\n",
    "    if not data:\n",
    "        raise ValueError(\"Data is empty\")\n",
    "\n",
    "    data[\"associated\"][\"spotId\"] = jetty_id\n",
    "    meta_df = pd.json_normalize(jetty_waves)\n",
    "    jetty_meta_df.drop(['permissions.violations', 'permissions.data', 'data.wave'], inplace=True, axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
